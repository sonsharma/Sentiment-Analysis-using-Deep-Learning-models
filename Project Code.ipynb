{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JdKCArUJg-p"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hji_eA7M3Spr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import json \n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4-Vp6uQJ3m1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "c83766b5-4f71-4f97-e025-696c36a3b8c7"
      },
      "source": [
        "downloaded = drive.CreateFile({'id': '1R0V2OuTW12oupYMk07RJJFOr-EqpVHTI'})\n",
        "downloaded.GetContentFile('Tweets.csv')\n",
        "df_orig = pd.read_csv('Tweets.csv')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1FHE2Zpp7i4I3XnuHqbqKygGh3RcMfJ0J'})\n",
        "downloaded.GetContentFile('dictionary.json')\n",
        "  \n",
        "f = open('dictionary.json',) \n",
        "data = json.load(f) \n",
        "\n",
        "df_orig.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>length</th>\n",
              "      <th>1_word</th>\n",
              "      <th>2_word</th>\n",
              "      <th>3_word</th>\n",
              "      <th>4_word</th>\n",
              "      <th>5_word</th>\n",
              "      <th>6_word</th>\n",
              "      <th>7_word</th>\n",
              "      <th>8_word</th>\n",
              "      <th>9_word</th>\n",
              "      <th>10_word</th>\n",
              "      <th>11_word</th>\n",
              "      <th>12_word</th>\n",
              "      <th>13_word</th>\n",
              "      <th>14_word</th>\n",
              "      <th>15_word</th>\n",
              "      <th>16_word</th>\n",
              "      <th>17_word</th>\n",
              "      <th>18_word</th>\n",
              "      <th>19_word</th>\n",
              "      <th>20_word</th>\n",
              "      <th>21_word</th>\n",
              "      <th>22_word</th>\n",
              "      <th>23_word</th>\n",
              "      <th>24_word</th>\n",
              "      <th>25_word</th>\n",
              "      <th>26_word</th>\n",
              "      <th>27_word</th>\n",
              "      <th>28_word</th>\n",
              "      <th>29_word</th>\n",
              "      <th>30_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>@virginamerica what @dhepburn said</td>\n",
              "      <td>4</td>\n",
              "      <td>13634</td>\n",
              "      <td>2106</td>\n",
              "      <td>10268</td>\n",
              "      <td>11928</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>@virginamerica plus youve added commercials to...</td>\n",
              "      <td>9</td>\n",
              "      <td>13634</td>\n",
              "      <td>1715</td>\n",
              "      <td>5248</td>\n",
              "      <td>7030</td>\n",
              "      <td>8653</td>\n",
              "      <td>4463</td>\n",
              "      <td>1903</td>\n",
              "      <td>2752</td>\n",
              "      <td>8770</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>@virginamerica i didnt today must mean i need ...</td>\n",
              "      <td>12</td>\n",
              "      <td>13634</td>\n",
              "      <td>8075</td>\n",
              "      <td>2858</td>\n",
              "      <td>2834</td>\n",
              "      <td>5694</td>\n",
              "      <td>10500</td>\n",
              "      <td>8075</td>\n",
              "      <td>3457</td>\n",
              "      <td>4463</td>\n",
              "      <td>7816</td>\n",
              "      <td>132</td>\n",
              "      <td>9075</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>@virginamerica its really aggressive to blast ...</td>\n",
              "      <td>17</td>\n",
              "      <td>13634</td>\n",
              "      <td>9067</td>\n",
              "      <td>4484</td>\n",
              "      <td>2406</td>\n",
              "      <td>4463</td>\n",
              "      <td>12531</td>\n",
              "      <td>10701</td>\n",
              "      <td>6923</td>\n",
              "      <td>8404</td>\n",
              "      <td>2958</td>\n",
              "      <td>11955</td>\n",
              "      <td>10016</td>\n",
              "      <td>10936</td>\n",
              "      <td>16644</td>\n",
              "      <td>10010</td>\n",
              "      <td>683</td>\n",
              "      <td>8325</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>@virginamerica and its a really big bad thing ...</td>\n",
              "      <td>10</td>\n",
              "      <td>13634</td>\n",
              "      <td>6149</td>\n",
              "      <td>9067</td>\n",
              "      <td>15211</td>\n",
              "      <td>4484</td>\n",
              "      <td>4383</td>\n",
              "      <td>9808</td>\n",
              "      <td>412</td>\n",
              "      <td>11230</td>\n",
              "      <td>6737</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ... 30_word\n",
              "0          0  ...       0\n",
              "1          1  ...       0\n",
              "2          0  ...       0\n",
              "3         -1  ...       0\n",
              "4         -1  ...       0\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUuEFc7JG7pC"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TjO704EGNRe"
      },
      "source": [
        "cols = ['sentiment', '1_word', '2_word', '3_word', '4_word', '5_word', '6_word',\n",
        "       '7_word', '8_word', '9_word', '10_word', '11_word', '12_word',\n",
        "       '13_word', '14_word', '15_word', '16_word', '17_word', '18_word',\n",
        "       '19_word', '20_word', '21_word', '22_word', '23_word', '24_word',\n",
        "       '25_word', '26_word', '27_word', '28_word', '29_word', '30_word']\n",
        "df_seq = pd.DataFrame(columns = cols)\n",
        "\n",
        "for i in range(df_orig.shape[0]):\n",
        "  lst=[df_orig.loc[i].sentiment]\n",
        "  processed_text = re.sub('[^A-Za-z0-9 @]+', '', df_orig.loc[i].text.lower())\n",
        "  lst.extend([data.get(w) for w in processed_text.split()])\n",
        "  lst.extend([0] * (31 - len(lst)))\n",
        "  df_seq = df_seq.append(pd.Series(lst, index = cols), ignore_index=True)\n",
        "df_seq = df_seq.fillna(0)\n",
        "df_seq = df_seq.astype('int32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAwxJbxvUfye"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PD6pzdlUway",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f6069cdc-06b6-40d9-f9a5-14ec14f77abc"
      },
      "source": [
        "df_seq.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>1_word</th>\n",
              "      <th>2_word</th>\n",
              "      <th>3_word</th>\n",
              "      <th>4_word</th>\n",
              "      <th>5_word</th>\n",
              "      <th>6_word</th>\n",
              "      <th>7_word</th>\n",
              "      <th>8_word</th>\n",
              "      <th>9_word</th>\n",
              "      <th>10_word</th>\n",
              "      <th>11_word</th>\n",
              "      <th>12_word</th>\n",
              "      <th>13_word</th>\n",
              "      <th>14_word</th>\n",
              "      <th>15_word</th>\n",
              "      <th>16_word</th>\n",
              "      <th>17_word</th>\n",
              "      <th>18_word</th>\n",
              "      <th>19_word</th>\n",
              "      <th>20_word</th>\n",
              "      <th>21_word</th>\n",
              "      <th>22_word</th>\n",
              "      <th>23_word</th>\n",
              "      <th>24_word</th>\n",
              "      <th>25_word</th>\n",
              "      <th>26_word</th>\n",
              "      <th>27_word</th>\n",
              "      <th>28_word</th>\n",
              "      <th>29_word</th>\n",
              "      <th>30_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10348</td>\n",
              "      <td>4290</td>\n",
              "      <td>10815</td>\n",
              "      <td>1667</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10348</td>\n",
              "      <td>10971</td>\n",
              "      <td>6259</td>\n",
              "      <td>13095</td>\n",
              "      <td>9335</td>\n",
              "      <td>8008</td>\n",
              "      <td>11667</td>\n",
              "      <td>8991</td>\n",
              "      <td>7120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>10348</td>\n",
              "      <td>8598</td>\n",
              "      <td>3623</td>\n",
              "      <td>2391</td>\n",
              "      <td>3090</td>\n",
              "      <td>4707</td>\n",
              "      <td>8598</td>\n",
              "      <td>4805</td>\n",
              "      <td>8008</td>\n",
              "      <td>8757</td>\n",
              "      <td>4718</td>\n",
              "      <td>6022</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>10348</td>\n",
              "      <td>14276</td>\n",
              "      <td>6398</td>\n",
              "      <td>7987</td>\n",
              "      <td>8008</td>\n",
              "      <td>11742</td>\n",
              "      <td>14871</td>\n",
              "      <td>2463</td>\n",
              "      <td>9054</td>\n",
              "      <td>6700</td>\n",
              "      <td>15758</td>\n",
              "      <td>14352</td>\n",
              "      <td>14887</td>\n",
              "      <td>13292</td>\n",
              "      <td>11658</td>\n",
              "      <td>2436</td>\n",
              "      <td>15493</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>10348</td>\n",
              "      <td>15348</td>\n",
              "      <td>14276</td>\n",
              "      <td>9717</td>\n",
              "      <td>6398</td>\n",
              "      <td>4289</td>\n",
              "      <td>6623</td>\n",
              "      <td>14735</td>\n",
              "      <td>16131</td>\n",
              "      <td>9329</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  1_word  2_word  3_word  ...  27_word  28_word  29_word  30_word\n",
              "0          0   10348    4290   10815  ...        0        0        0        0\n",
              "1          1   10348   10971    6259  ...        0        0        0        0\n",
              "2          0   10348    8598    3623  ...        0        0        0        0\n",
              "3         -1   10348   14276    6398  ...        0        0        0        0\n",
              "4         -1   10348   15348   14276  ...        0        0        0        0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW0xrRDq1O94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "727d55cf-b17f-4e73-ecd7-23ccc85f39c6"
      },
      "source": [
        "df_seq.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentiment', '1_word', '2_word', '3_word', '4_word', '5_word', '6_word',\n",
              "       '7_word', '8_word', '9_word', '10_word', '11_word', '12_word',\n",
              "       '13_word', '14_word', '15_word', '16_word', '17_word', '18_word',\n",
              "       '19_word', '20_word', '21_word', '22_word', '23_word', '24_word',\n",
              "       '25_word', '26_word', '27_word', '28_word', '29_word', '30_word'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXH0PT2SUzq6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "43d9e723-f6eb-4f3e-fbe0-fa4741a496c5"
      },
      "source": [
        "(tr_unique, tr_counts) = np.unique(df_seq['sentiment'].values, return_counts=True)\n",
        "perc = tr_counts/np.sum(tr_counts)\n",
        "pd.DataFrame((tr_counts,perc), columns=tr_unique,index=[\"Count\",\"Percentage\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-1</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Count</th>\n",
              "      <td>9058.000000</td>\n",
              "      <td>3080.000000</td>\n",
              "      <td>2355.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Percentage</th>\n",
              "      <td>0.624991</td>\n",
              "      <td>0.212516</td>\n",
              "      <td>0.162492</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     -1            0            1\n",
              "Count       9058.000000  3080.000000  2355.000000\n",
              "Percentage     0.624991     0.212516     0.162492"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzgBR5rfaeL8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a9e17438-fcc6-48e7-9656-f29695616742"
      },
      "source": [
        "xTrain, xTest, yTrain, yTest = train_test_split(df_seq.drop(columns = ['sentiment']), df_seq['sentiment']+1, test_size = 0.2, \n",
        "                                                    random_state = 0, shuffle = True)\n",
        "\n",
        "xTr_tensor = torch.Tensor(xTrain.values)\n",
        "xTs_tensor = torch.Tensor(xTest.values)\n",
        "\n",
        "yTr_tensor = torch.Tensor(yTrain.values)\n",
        "yTs_tensor = torch.Tensor(yTest.values)\n",
        "\n",
        "train = torch.utils.data.TensorDataset(xTr_tensor, yTr_tensor)\n",
        "train_loaders = torch.utils.data.DataLoader(train, batch_size=100, shuffle=True)\n",
        "(tr_unique, tr_counts) = np.unique(yTrain.values, return_counts=True)\n",
        "\n",
        "\n",
        "test = torch.utils.data.TensorDataset(xTs_tensor,yTs_tensor)\n",
        "test_loaders = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True)\n",
        "(ts_unique, ts_counts) = np.unique(yTest.values, return_counts=True)\n",
        "print(\"Pandas: \\nTrain Label Unique Coumts Labels: {}, Counts: {} \\nTest Label Unique Coumts Labels: {}, Counts: {}  \"\n",
        "          .format(tr_unique, tr_counts,ts_unique,ts_counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pandas: \n",
            "Train Label Unique Coumts Labels: [0 1 2], Counts: [7258 2431 1905] \n",
            "Test Label Unique Coumts Labels: [0 1 2], Counts: [1800  649  450]  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWf_6_eMhjzt"
      },
      "source": [
        "# Set up your device \n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYy4W064wJag"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Simple baseline model (using sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s1UIkIIKQS4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5948d5bb-66ce-492a-ba86-0ee2483be13f"
      },
      "source": [
        "#simple baseline model (using sklearn)\n",
        "X = df_seq.loc[:,df_seq.columns!='sentiment']\n",
        "Y = df_seq['sentiment']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = np.random.seed(42))\n",
        "st = time.time()\n",
        "model_binary = LogisticRegression(solver='liblinear')\n",
        "model_binary.fit(X_train, Y_train)\n",
        "print(accuracy_score(model_binary.predict(X_test), Y_test)*100)\n",
        "en = time.time()\n",
        "print('Total Time to Execute: {:.0f} Seconds'.format(en-st)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63.504656778199376\n",
            "Total Time to Execute: 0 Seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK8rn96rwRcc"
      },
      "source": [
        "# Simple baseline LR model (using pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to2ya7YKUMjL"
      },
      "source": [
        "#simple baseline LR model (using pytorch)\n",
        "\n",
        "X,Y = train_test_split(df_seq, test_size=0.2, random_state = np.random.seed(42))\n",
        "batch_size=100\n",
        "\n",
        "train_dataset = TensorDataset( Tensor(X.loc[:,X.columns!='sentiment'].values), Tensor(X['sentiment'].values) )\n",
        "test_dataset = TensorDataset( Tensor(Y.loc[:,Y.columns!='sentiment'].values), Tensor(Y['sentiment'].values) )\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBv_665vPPcq"
      },
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.linear(x)\n",
        "        return outputs\n",
        "\n",
        "n_iters = 3000\n",
        "epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "input_dim = 30\n",
        "output_dim = 3  #{-1, 0, 1}\n",
        "lr_rate = 0.001\n",
        "\n",
        "LRmodel = LogisticRegression(input_dim, output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZcD-TYUYKYM"
      },
      "source": [
        "# Training model on training data\n",
        "criterion = torch.nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.SGD(LRmodel.parameters(), lr=lr_rate)\n",
        "iter = 0\n",
        "st = time.time()\n",
        "for epoch in range(int(epochs)):\n",
        "    for i in enumerate(train_loader):\n",
        "        inputs = Variable(i[1][0]) #features\n",
        "        labels = Variable(i[1][1])\n",
        "        optimizer.zero_grad()\n",
        "        #print(epoch,inputs,\"\\n------\\n\")\n",
        "        outputs = LRmodel(inputs)\n",
        "        #print(outputs.squeeze().shape, (labels.long()+1).shape,\"\\n------\\n\")\n",
        "        loss = criterion(outputs.squeeze(), labels.long()+1)\n",
        "        #print(\"epoch\",epoch,loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZTxpyCFnbyg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0bd1e277-af7f-4655-a61d-9e77acab0a0d"
      },
      "source": [
        "# Validating on the test data\n",
        "correct = 0\n",
        "total = 0\n",
        "iter_test = 0\n",
        "for j in enumerate(test_loader):\n",
        "    iter_test += 1\n",
        "    inputs = Variable(j[1][0]) #features\n",
        "    labels = Variable(j[1][1])\n",
        "    outputs = LRmodel(inputs)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Total number of labels\n",
        "    total += labels.size(0)\n",
        "\n",
        "    # Total correct predictions\n",
        "    correct += (predicted == labels.long()+1).sum()\n",
        "\n",
        "accuracy = 100 * (correct.item() / total)\n",
        "print(accuracy)\n",
        "en = time.time()\n",
        "print('Total Time to Execute: {:.0f} Seconds'.format(en-st)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62.36633321835116\n",
            "Total Time to Execute: 4 Seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxLZWKbXwVTN"
      },
      "source": [
        "# Deep Learning Model - CNN (using pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyxStgK7wbIZ"
      },
      "source": [
        "class SimpleCNN(torch.nn.Module):    \n",
        "    def __init__(self,input_dim, output_dim):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1_1 = torch.nn.Conv1d(input_dim, batch_size, kernel_size=2, stride=1, padding=2)\n",
        "        self.conv1_2 = torch.nn.Conv1d(input_dim, batch_size, kernel_size=1, stride=1)\n",
        "        self.pool = torch.nn.MaxPool1d(kernel_size=3)\n",
        "        self.fc1 = torch.nn.Linear(batch_size, output_dim)\n",
        "        self.fc2 = torch.nn.Linear(output_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1_1(x))\n",
        "        #x1 = F.relu(self.conv1_2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, batch_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return(x)\n",
        "\n",
        "n_iters = 3000\n",
        "epochs = 50\n",
        "input_dim = 30\n",
        "output_dim = 3  #{-1, 0, 1}\n",
        "lr_rate = 0.01\n",
        "\n",
        "CNNmodel = SimpleCNN(input_dim, output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIIwMjRd1ZSR"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(CNNmodel.parameters(), lr=lr_rate)\n",
        "st = time.time()\n",
        "for epoch in range(int(epochs)):\n",
        "    for i in enumerate(train_loader):\n",
        "        inputs = Variable(i[1][0]).unsqueeze(2) #features\n",
        "        labels = Variable(i[1][1])\n",
        "        optimizer.zero_grad()\n",
        "        outputs = CNNmodel(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.long()+1)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2AxQIT1JALq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "59fed04f-33cb-4c09-ba9e-1e0326338f70"
      },
      "source": [
        "# Validating on the test data\n",
        "correct = 0\n",
        "total = 0\n",
        "iter_test = 0\n",
        "total_val_loss = 0\n",
        "for j in enumerate(test_loader):\n",
        "    iter_test += 1\n",
        "    inputs = Variable(j[1][0]).unsqueeze(2) #features\n",
        "    labels = Variable(j[1][1])\n",
        "    outputs = CNNmodel(inputs)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels.long()+1).sum()\n",
        "accuracy = 100 * (correct.item() / total)\n",
        "print(accuracy)\n",
        "en = time.time()\n",
        "print('Total Time to Execute: {:.0f} Seconds'.format(en-st)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64.29803380476025\n",
            "Total Time to Execute: 21 Seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UquVblqO9N0"
      },
      "source": [
        "# Train - Test Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEQxw5wDh7yv"
      },
      "source": [
        "# Train and Test Utils.\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, log_interval = 100, float_dtype=False):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    num_correct = 0\n",
        "    # Loop through data points\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    \n",
        "        # Send data and target to device\n",
        "        if float_dtype:\n",
        "            data, target = data.to(device), target.flatten().long().to(device)\n",
        "            \n",
        "        else:\n",
        "            data, target = data.long().to(device), target.flatten().long().to(device)\n",
        "        \n",
        "        # Zero out the ortimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Pass data through model\n",
        "        output = model(data)\n",
        "        \n",
        "        # Compute the negative log likelihood loss\n",
        "        \n",
        "        loss = F.nll_loss(output, target)\n",
        "        \n",
        "        # Backpropagate loss\n",
        "        loss.backward()\n",
        "        \n",
        "        # Make a step with the optimizer\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Get predictions from the model for each data point\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        # Add number of correct predictions to total num_correct \n",
        "        num_correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "        \n",
        "        # Print loss \n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        \n",
        "    \"\"\"\n",
        "        # Print loss \n",
        "    print('\\ Train Accuracy ({:.0f}%)\\n'.format(\n",
        "        100. * num_correct / len(train_loader.dataset)))\n",
        "    acc = 100. * num_correct / len(train_loader.dataset)\n",
        "    \"\"\"\n",
        "    acc = 100. * num_correct / len(train_loader.dataset)\n",
        "    return acc\n",
        "\n",
        "            \n",
        "# Define test method\n",
        "def test(model, device, test_loader, float_dtype = False  ):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # Variable for the total loss \n",
        "    test_loss = 0\n",
        "    # Counter for the correct predictions\n",
        "    num_correct = 0\n",
        "    \n",
        "    # don't need autograd for eval\n",
        "    with torch.no_grad():\n",
        "        # Loop through data points\n",
        "        for data, target in test_loader:\n",
        "        \n",
        "            # Send data to device\n",
        "            if float_dtype:\n",
        "                data, target = data.to(device), target.flatten().long().to(device)\n",
        "            \n",
        "            else:\n",
        "                data, target = data.long().to(device), target.flatten().long().to(device)\n",
        "            # Pass data through model\n",
        "            output = model(data) \n",
        "            \n",
        "            # Compute the negative log likelihood loss with reduction='sum' and add to total test_loss\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            \n",
        "            # Get predictions from the model for each data point\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            # Add number of correct predictions to total num_correct \n",
        "            num_correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "    # Compute the average test_loss\n",
        "    avg_test_loss = test_loss /len(test_loader.dataset)\n",
        "    \n",
        "    \"\"\"\n",
        "    # Print loss \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
        "        100. * num_correct / len(test_loader.dataset)))\n",
        "    acc = 100. * num_correct / len(test_loader.dataset)\n",
        "    \"\"\"\n",
        "    acc = 100. * num_correct / len(test_loader.dataset)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MniezaRfoO8e"
      },
      "source": [
        "# RNN Based Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpRXDgUXPCDj"
      },
      "source": [
        "# Define Models\n",
        "\n",
        "class FC2Layer(nn.Module):\n",
        "    def __init__(self, input_size, n_hidden, output_size):\n",
        "        super(FC2Layer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, n_hidden), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(n_hidden, n_hidden), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(n_hidden, output_size), \n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_size)\n",
        "        return self.network(x)\n",
        "    \n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):        \n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text.T)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        decoded = self.fc(hidden.squeeze(0))\n",
        "        \n",
        "        return F.log_softmax(decoded, dim=1)\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text.T))\n",
        "        packed_output, (hidden, cell) = self.rnn(embedded)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                       \n",
        "        decoded = self.fc(hidden)\n",
        "        return  F.log_softmax(decoded, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBjYp_cvorul"
      },
      "source": [
        "# FCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U77uw-u9iG6r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e896da2d-dc51-42be-b177-493b7a07494f"
      },
      "source": [
        "import time\n",
        "input_size = 30\n",
        "n_hidden = 100\n",
        "output_size = 3\n",
        "\n",
        "# Deifne model and sent to device\n",
        "model = FC2Layer(input_size, n_hidden, output_size)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer: SGD with learning rate of 1e-2 and momentum of 0.5\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "tr_acc_list = list()\n",
        "ts_acc_list = list()\n",
        "# Training loop with 10 epochs\n",
        "st = time.time()\n",
        "for epoch in range(1, 30+ 1):\n",
        "    \n",
        "    # Train model\n",
        "    tr_acc = train(model,device,train_loaders,optimizer,epoch,float_dtype=True)\n",
        "    tr_acc_list.append(tr_acc)\n",
        "    \n",
        "    # Test model\n",
        "    ts_acc = test(model,device,test_loaders, float_dtype=True)\n",
        "    ts_acc_list.append(ts_acc)\n",
        "    \n",
        "print(\"******FINAL Accuracy at Last Epoch ******\")\n",
        "print('\\nTrain set:  Accuracy: {}, \\nTest set:  Accuracy: {}\\n'.format(\n",
        "        tr_acc,ts_acc ))\n",
        "en = time.time()\n",
        "print('Total Time to Execute: {:.0f} Seconds'.format(en-st)) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/11594 (0%)]\tLoss: 267.826416\n",
            "Train Epoch: 1 [10000/11594 (86%)]\tLoss: 0.914237\n",
            "Train Epoch: 2 [0/11594 (0%)]\tLoss: 0.852147\n",
            "Train Epoch: 2 [10000/11594 (86%)]\tLoss: 0.952228\n",
            "Train Epoch: 3 [0/11594 (0%)]\tLoss: 0.947160\n",
            "Train Epoch: 3 [10000/11594 (86%)]\tLoss: 0.957642\n",
            "Train Epoch: 4 [0/11594 (0%)]\tLoss: 0.862415\n",
            "Train Epoch: 4 [10000/11594 (86%)]\tLoss: 0.865937\n",
            "Train Epoch: 5 [0/11594 (0%)]\tLoss: 0.893779\n",
            "Train Epoch: 5 [10000/11594 (86%)]\tLoss: 0.824448\n",
            "Train Epoch: 6 [0/11594 (0%)]\tLoss: 0.872060\n",
            "Train Epoch: 6 [10000/11594 (86%)]\tLoss: 0.954461\n",
            "Train Epoch: 7 [0/11594 (0%)]\tLoss: 0.902083\n",
            "Train Epoch: 7 [10000/11594 (86%)]\tLoss: 0.863217\n",
            "Train Epoch: 8 [0/11594 (0%)]\tLoss: 0.815014\n",
            "Train Epoch: 8 [10000/11594 (86%)]\tLoss: 0.914238\n",
            "Train Epoch: 9 [0/11594 (0%)]\tLoss: 0.948782\n",
            "Train Epoch: 9 [10000/11594 (86%)]\tLoss: 0.938451\n",
            "Train Epoch: 10 [0/11594 (0%)]\tLoss: 1.050042\n",
            "Train Epoch: 10 [10000/11594 (86%)]\tLoss: 0.924447\n",
            "Train Epoch: 11 [0/11594 (0%)]\tLoss: 0.923169\n",
            "Train Epoch: 11 [10000/11594 (86%)]\tLoss: 1.037561\n",
            "Train Epoch: 12 [0/11594 (0%)]\tLoss: 0.847634\n",
            "Train Epoch: 12 [10000/11594 (86%)]\tLoss: 0.979709\n",
            "Train Epoch: 13 [0/11594 (0%)]\tLoss: 0.925591\n",
            "Train Epoch: 13 [10000/11594 (86%)]\tLoss: 0.915579\n",
            "Train Epoch: 14 [0/11594 (0%)]\tLoss: 1.007226\n",
            "Train Epoch: 14 [10000/11594 (86%)]\tLoss: 0.966361\n",
            "Train Epoch: 15 [0/11594 (0%)]\tLoss: 0.889615\n",
            "Train Epoch: 15 [10000/11594 (86%)]\tLoss: 0.947645\n",
            "Train Epoch: 16 [0/11594 (0%)]\tLoss: 0.920589\n",
            "Train Epoch: 16 [10000/11594 (86%)]\tLoss: 0.724104\n",
            "Train Epoch: 17 [0/11594 (0%)]\tLoss: 0.920503\n",
            "Train Epoch: 17 [10000/11594 (86%)]\tLoss: 1.020449\n",
            "Train Epoch: 18 [0/11594 (0%)]\tLoss: 0.883284\n",
            "Train Epoch: 18 [10000/11594 (86%)]\tLoss: 0.925481\n",
            "Train Epoch: 19 [0/11594 (0%)]\tLoss: 0.907380\n",
            "Train Epoch: 19 [10000/11594 (86%)]\tLoss: 0.932534\n",
            "Train Epoch: 20 [0/11594 (0%)]\tLoss: 0.914602\n",
            "Train Epoch: 20 [10000/11594 (86%)]\tLoss: 0.894048\n",
            "Train Epoch: 21 [0/11594 (0%)]\tLoss: 0.956128\n",
            "Train Epoch: 21 [10000/11594 (86%)]\tLoss: 1.107877\n",
            "Train Epoch: 22 [0/11594 (0%)]\tLoss: 0.947287\n",
            "Train Epoch: 22 [10000/11594 (86%)]\tLoss: 0.942594\n",
            "Train Epoch: 23 [0/11594 (0%)]\tLoss: 0.889793\n",
            "Train Epoch: 23 [10000/11594 (86%)]\tLoss: 0.991089\n",
            "Train Epoch: 24 [0/11594 (0%)]\tLoss: 0.923909\n",
            "Train Epoch: 24 [10000/11594 (86%)]\tLoss: 0.896228\n",
            "Train Epoch: 25 [0/11594 (0%)]\tLoss: 0.843846\n",
            "Train Epoch: 25 [10000/11594 (86%)]\tLoss: 0.872718\n",
            "Train Epoch: 26 [0/11594 (0%)]\tLoss: 0.920502\n",
            "Train Epoch: 26 [10000/11594 (86%)]\tLoss: 0.971573\n",
            "Train Epoch: 27 [0/11594 (0%)]\tLoss: 0.907732\n",
            "Train Epoch: 27 [10000/11594 (86%)]\tLoss: 0.980443\n",
            "Train Epoch: 28 [0/11594 (0%)]\tLoss: 0.836428\n",
            "Train Epoch: 28 [10000/11594 (86%)]\tLoss: 0.943050\n",
            "Train Epoch: 29 [0/11594 (0%)]\tLoss: 0.984628\n",
            "Train Epoch: 29 [10000/11594 (86%)]\tLoss: 0.919249\n",
            "Train Epoch: 30 [0/11594 (0%)]\tLoss: 1.022001\n",
            "Train Epoch: 30 [10000/11594 (86%)]\tLoss: 0.895589\n",
            "******FINAL Accuracy at Last Epoch ******\n",
            "\n",
            "Train set:  Accuracy: 62.60134552354666, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Total Time to Execute: 8 Seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1jJ7l5hopUr"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md0xtlVxPD6b"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpcubb49rvGj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f6e4c85-c6f0-4327-8070-0505b7a189c5"
      },
      "source": [
        "input_dim = 17201\n",
        "embedding_dim = 30\n",
        "hidden_dim = 512\n",
        "ouput_dim = 3\n",
        "\n",
        "model = RNN(input_dim, embedding_dim, hidden_dim, ouput_dim)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer: SGD with learning rate of 1e-2 and momentum of 0.5\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "tr_acc_list = list()\n",
        "ts_acc_list = list()\n",
        "st = time.time()\n",
        "# Training loop with 10 epochs\n",
        "for epoch in range(1, 50 + 1):\n",
        "    \n",
        "    # Train model\n",
        "    tr_acc = train(model,device,train_loaders,optimizer,epoch)\n",
        "    tr_acc_list.append(tr_acc)\n",
        "    # Test model\n",
        "    ts_acc = test(model,device,test_loaders)\n",
        "    ts_acc_list.append(ts_acc)\n",
        "    print('\\nTrain set:  Accuracy: {}, \\nTest set:  Accuracy: {}\\n'.format(\n",
        "        tr_acc,ts_acc ))\n",
        "print(\"******FINAL Accuracy at Last Epoch ******\")\n",
        "print('\\nTrain set:  Accuracy: {}, \\nTest set:  Accuracy: {}\\n'.format(\n",
        "        tr_acc,ts_acc ))\n",
        "en = time.time()\n",
        "print('Total Time to Execute: {:.0f} Seconds'.format(en-st)) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/11594 (0%)]\tLoss: 1.115612\n",
            "Train Epoch: 1 [10000/11594 (86%)]\tLoss: 0.901312\n",
            "\n",
            "Train set:  Accuracy: 61.97170950491633, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Train Epoch: 2 [0/11594 (0%)]\tLoss: 0.931517\n",
            "Train Epoch: 2 [10000/11594 (86%)]\tLoss: 0.966265\n",
            "\n",
            "Train set:  Accuracy: 62.60134552354666, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Train Epoch: 3 [0/11594 (0%)]\tLoss: 0.996427\n",
            "Train Epoch: 3 [10000/11594 (86%)]\tLoss: 0.842237\n",
            "\n",
            "Train set:  Accuracy: 62.60134552354666, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Train Epoch: 4 [0/11594 (0%)]\tLoss: 0.896809\n",
            "Train Epoch: 4 [10000/11594 (86%)]\tLoss: 0.855384\n",
            "\n",
            "Train set:  Accuracy: 63.01535276867345, \n",
            "Test set:  Accuracy: 64.05657123145913\n",
            "\n",
            "Train Epoch: 5 [0/11594 (0%)]\tLoss: 0.868890\n",
            "Train Epoch: 5 [10000/11594 (86%)]\tLoss: 0.827404\n",
            "\n",
            "Train set:  Accuracy: 63.53286182508194, \n",
            "Test set:  Accuracy: 64.67747499137634\n",
            "\n",
            "Train Epoch: 6 [0/11594 (0%)]\tLoss: 0.782636\n",
            "Train Epoch: 6 [10000/11594 (86%)]\tLoss: 0.924703\n",
            "\n",
            "Train set:  Accuracy: 63.877867862687594, \n",
            "Test set:  Accuracy: 64.26353915143153\n",
            "\n",
            "Train Epoch: 7 [0/11594 (0%)]\tLoss: 0.719214\n",
            "Train Epoch: 7 [10000/11594 (86%)]\tLoss: 0.812950\n",
            "\n",
            "Train set:  Accuracy: 63.903743315508024, \n",
            "Test set:  Accuracy: 63.60814073818558\n",
            "\n",
            "Train Epoch: 8 [0/11594 (0%)]\tLoss: 0.879824\n",
            "Train Epoch: 8 [10000/11594 (86%)]\tLoss: 0.923062\n",
            "\n",
            "Train set:  Accuracy: 63.9813696739693, \n",
            "Test set:  Accuracy: 63.19420489824077\n",
            "\n",
            "Train Epoch: 9 [0/11594 (0%)]\tLoss: 0.877846\n",
            "Train Epoch: 9 [10000/11594 (86%)]\tLoss: 0.787012\n",
            "\n",
            "Train set:  Accuracy: 64.72313265482146, \n",
            "Test set:  Accuracy: 65.33287340462229\n",
            "\n",
            "Train Epoch: 10 [0/11594 (0%)]\tLoss: 0.802359\n",
            "Train Epoch: 10 [10000/11594 (86%)]\tLoss: 0.690297\n",
            "\n",
            "Train set:  Accuracy: 65.26651716405037, \n",
            "Test set:  Accuracy: 63.88409796481545\n",
            "\n",
            "Train Epoch: 11 [0/11594 (0%)]\tLoss: 0.871682\n",
            "Train Epoch: 11 [10000/11594 (86%)]\tLoss: 0.775235\n",
            "\n",
            "Train set:  Accuracy: 64.83525961704329, \n",
            "Test set:  Accuracy: 64.57399103139014\n",
            "\n",
            "Train Epoch: 12 [0/11594 (0%)]\tLoss: 0.930581\n",
            "Train Epoch: 12 [10000/11594 (86%)]\tLoss: 0.854211\n",
            "\n",
            "Train set:  Accuracy: 65.27514231499052, \n",
            "Test set:  Accuracy: 61.676440151776475\n",
            "\n",
            "Train Epoch: 13 [0/11594 (0%)]\tLoss: 0.783422\n",
            "Train Epoch: 13 [10000/11594 (86%)]\tLoss: 0.812035\n",
            "\n",
            "Train set:  Accuracy: 65.24926686217009, \n",
            "Test set:  Accuracy: 64.98792687133495\n",
            "\n",
            "Train Epoch: 14 [0/11594 (0%)]\tLoss: 0.792209\n",
            "Train Epoch: 14 [10000/11594 (86%)]\tLoss: 0.804838\n",
            "\n",
            "Train set:  Accuracy: 65.74090046575814, \n",
            "Test set:  Accuracy: 66.12625043118317\n",
            "\n",
            "Train Epoch: 15 [0/11594 (0%)]\tLoss: 0.798276\n",
            "Train Epoch: 15 [10000/11594 (86%)]\tLoss: 0.781020\n",
            "\n",
            "Train set:  Accuracy: 65.56839744695532, \n",
            "Test set:  Accuracy: 65.22938944463608\n",
            "\n",
            "Train Epoch: 16 [0/11594 (0%)]\tLoss: 0.845504\n",
            "Train Epoch: 16 [10000/11594 (86%)]\tLoss: 0.853980\n",
            "\n",
            "Train set:  Accuracy: 65.88752803174056, \n",
            "Test set:  Accuracy: 65.67781993790962\n",
            "\n",
            "Train Epoch: 17 [0/11594 (0%)]\tLoss: 0.798489\n",
            "Train Epoch: 17 [10000/11594 (86%)]\tLoss: 0.949386\n",
            "\n",
            "Train set:  Accuracy: 65.99102984302225, \n",
            "Test set:  Accuracy: 65.71231459123835\n",
            "\n",
            "Train Epoch: 18 [0/11594 (0%)]\tLoss: 0.808850\n",
            "Train Epoch: 18 [10000/11594 (86%)]\tLoss: 0.883343\n",
            "\n",
            "Train set:  Accuracy: 65.95652923926168, \n",
            "Test set:  Accuracy: 65.47085201793722\n",
            "\n",
            "Train Epoch: 19 [0/11594 (0%)]\tLoss: 0.718598\n",
            "Train Epoch: 19 [10000/11594 (86%)]\tLoss: 0.888944\n",
            "\n",
            "Train set:  Accuracy: 66.03415559772296, \n",
            "Test set:  Accuracy: 65.53984132459469\n",
            "\n",
            "Train Epoch: 20 [0/11594 (0%)]\tLoss: 0.794131\n",
            "Train Epoch: 20 [10000/11594 (86%)]\tLoss: 0.948891\n",
            "\n",
            "Train set:  Accuracy: 66.11178195618423, \n",
            "Test set:  Accuracy: 65.16040013797861\n",
            "\n",
            "Train Epoch: 21 [0/11594 (0%)]\tLoss: 0.715384\n",
            "Train Epoch: 21 [10000/11594 (86%)]\tLoss: 0.709023\n",
            "\n",
            "Train set:  Accuracy: 66.20665861652579, \n",
            "Test set:  Accuracy: 65.88478785788203\n",
            "\n",
            "Train Epoch: 22 [0/11594 (0%)]\tLoss: 0.737867\n",
            "Train Epoch: 22 [10000/11594 (86%)]\tLoss: 0.772076\n",
            "\n",
            "Train set:  Accuracy: 66.24115922028635, \n",
            "Test set:  Accuracy: 65.9537771645395\n",
            "\n",
            "Train Epoch: 23 [0/11594 (0%)]\tLoss: 0.900707\n",
            "Train Epoch: 23 [10000/11594 (86%)]\tLoss: 0.684860\n",
            "\n",
            "Train set:  Accuracy: 66.68104191823357, \n",
            "Test set:  Accuracy: 66.57468092445671\n",
            "\n",
            "Train Epoch: 24 [0/11594 (0%)]\tLoss: 0.763324\n",
            "Train Epoch: 24 [10000/11594 (86%)]\tLoss: 0.795893\n",
            "\n",
            "Train set:  Accuracy: 66.15490771088494, \n",
            "Test set:  Accuracy: 64.26353915143153\n",
            "\n",
            "Train Epoch: 25 [0/11594 (0%)]\tLoss: 0.808941\n",
            "Train Epoch: 25 [10000/11594 (86%)]\tLoss: 0.730106\n",
            "\n",
            "Train set:  Accuracy: 66.51716405037088, \n",
            "Test set:  Accuracy: 66.3677130044843\n",
            "\n",
            "Train Epoch: 26 [0/11594 (0%)]\tLoss: 0.916995\n",
            "Train Epoch: 26 [10000/11594 (86%)]\tLoss: 0.731746\n",
            "\n",
            "Train set:  Accuracy: 66.4999137484906, \n",
            "Test set:  Accuracy: 67.05760607105898\n",
            "\n",
            "Train Epoch: 27 [0/11594 (0%)]\tLoss: 0.771040\n",
            "Train Epoch: 27 [10000/11594 (86%)]\tLoss: 0.716465\n",
            "\n",
            "Train set:  Accuracy: 66.57754010695187, \n",
            "Test set:  Accuracy: 66.64367023111417\n",
            "\n",
            "Train Epoch: 28 [0/11594 (0%)]\tLoss: 0.723830\n",
            "Train Epoch: 28 [10000/11594 (86%)]\tLoss: 0.758302\n",
            "\n",
            "Train set:  Accuracy: 66.8966706917371, \n",
            "Test set:  Accuracy: 65.9537771645395\n",
            "\n",
            "Train Epoch: 29 [0/11594 (0%)]\tLoss: 0.779624\n",
            "Train Epoch: 29 [10000/11594 (86%)]\tLoss: 0.832182\n",
            "\n",
            "Train set:  Accuracy: 66.73279282387442, \n",
            "Test set:  Accuracy: 65.6433252845809\n",
            "\n",
            "Train Epoch: 30 [0/11594 (0%)]\tLoss: 0.780839\n",
            "Train Epoch: 30 [10000/11594 (86%)]\tLoss: 0.751577\n",
            "\n",
            "Train set:  Accuracy: 66.8966706917371, \n",
            "Test set:  Accuracy: 66.22973439116937\n",
            "\n",
            "Train Epoch: 31 [0/11594 (0%)]\tLoss: 0.850721\n",
            "Train Epoch: 31 [10000/11594 (86%)]\tLoss: 0.831577\n",
            "\n",
            "Train set:  Accuracy: 67.02604795583923, \n",
            "Test set:  Accuracy: 65.91928251121077\n",
            "\n",
            "Train Epoch: 32 [0/11594 (0%)]\tLoss: 0.748465\n",
            "Train Epoch: 32 [10000/11594 (86%)]\tLoss: 0.724696\n",
            "\n",
            "Train set:  Accuracy: 66.94842159737796, \n",
            "Test set:  Accuracy: 67.09210072438772\n",
            "\n",
            "Train Epoch: 33 [0/11594 (0%)]\tLoss: 0.798858\n",
            "Train Epoch: 33 [10000/11594 (86%)]\tLoss: 0.800181\n",
            "\n",
            "Train set:  Accuracy: 67.13817491806107, \n",
            "Test set:  Accuracy: 67.16109003104519\n",
            "\n",
            "Train Epoch: 34 [0/11594 (0%)]\tLoss: 0.784436\n",
            "Train Epoch: 34 [10000/11594 (86%)]\tLoss: 0.852277\n",
            "\n",
            "Train set:  Accuracy: 66.87942038985682, \n",
            "Test set:  Accuracy: 66.74715419110038\n",
            "\n",
            "Train Epoch: 35 [0/11594 (0%)]\tLoss: 0.739831\n",
            "Train Epoch: 35 [10000/11594 (86%)]\tLoss: 0.808055\n",
            "\n",
            "Train set:  Accuracy: 67.32792823874418, \n",
            "Test set:  Accuracy: 67.43704725767506\n",
            "\n",
            "Train Epoch: 36 [0/11594 (0%)]\tLoss: 0.714325\n",
            "Train Epoch: 36 [10000/11594 (86%)]\tLoss: 0.781875\n",
            "\n",
            "Train set:  Accuracy: 67.36242884250474, \n",
            "Test set:  Accuracy: 67.33356329768885\n",
            "\n",
            "Train Epoch: 37 [0/11594 (0%)]\tLoss: 0.869641\n",
            "Train Epoch: 37 [10000/11594 (86%)]\tLoss: 0.740036\n",
            "\n",
            "Train set:  Accuracy: 67.59530791788856, \n",
            "Test set:  Accuracy: 67.19558468437393\n",
            "\n",
            "Train Epoch: 38 [0/11594 (0%)]\tLoss: 0.717219\n",
            "Train Epoch: 38 [10000/11594 (86%)]\tLoss: 0.679827\n",
            "\n",
            "Train set:  Accuracy: 67.46593065378644, \n",
            "Test set:  Accuracy: 67.6785098309762\n",
            "\n",
            "Train Epoch: 39 [0/11594 (0%)]\tLoss: 0.759075\n",
            "Train Epoch: 39 [10000/11594 (86%)]\tLoss: 0.670930\n",
            "\n",
            "Train set:  Accuracy: 67.81956184233223, \n",
            "Test set:  Accuracy: 65.6433252845809\n",
            "\n",
            "Train Epoch: 40 [0/11594 (0%)]\tLoss: 0.803086\n",
            "Train Epoch: 40 [10000/11594 (86%)]\tLoss: 0.654618\n",
            "\n",
            "Train set:  Accuracy: 67.560807314128, \n",
            "Test set:  Accuracy: 67.40255260434633\n",
            "\n",
            "Train Epoch: 41 [0/11594 (0%)]\tLoss: 0.789967\n",
            "Train Epoch: 41 [10000/11594 (86%)]\tLoss: 0.807679\n",
            "\n",
            "Train set:  Accuracy: 67.99206486113506, \n",
            "Test set:  Accuracy: 67.4715419110038\n",
            "\n",
            "Train Epoch: 42 [0/11594 (0%)]\tLoss: 0.712796\n",
            "Train Epoch: 42 [10000/11594 (86%)]\tLoss: 0.749357\n",
            "\n",
            "Train set:  Accuracy: 67.67293427634984, \n",
            "Test set:  Accuracy: 67.33356329768885\n",
            "\n",
            "Train Epoch: 43 [0/11594 (0%)]\tLoss: 0.792884\n",
            "Train Epoch: 43 [10000/11594 (86%)]\tLoss: 0.720426\n",
            "\n",
            "Train set:  Accuracy: 67.9144385026738, \n",
            "Test set:  Accuracy: 66.43670231114177\n",
            "\n",
            "Train Epoch: 44 [0/11594 (0%)]\tLoss: 0.800326\n",
            "Train Epoch: 44 [10000/11594 (86%)]\tLoss: 0.675638\n",
            "\n",
            "Train set:  Accuracy: 67.76781093669139, \n",
            "Test set:  Accuracy: 67.7819937909624\n",
            "\n",
            "Train Epoch: 45 [0/11594 (0%)]\tLoss: 0.794853\n",
            "Train Epoch: 45 [10000/11594 (86%)]\tLoss: 0.828958\n",
            "\n",
            "Train set:  Accuracy: 67.90581335173366, \n",
            "Test set:  Accuracy: 66.85063815108659\n",
            "\n",
            "Train Epoch: 46 [0/11594 (0%)]\tLoss: 0.660792\n",
            "Train Epoch: 46 [10000/11594 (86%)]\tLoss: 0.684525\n",
            "\n",
            "Train set:  Accuracy: 68.15594272899776, \n",
            "Test set:  Accuracy: 68.57537081752328\n",
            "\n",
            "Train Epoch: 47 [0/11594 (0%)]\tLoss: 0.732486\n",
            "Train Epoch: 47 [10000/11594 (86%)]\tLoss: 0.670360\n",
            "\n",
            "Train set:  Accuracy: 68.40607210626186, \n",
            "Test set:  Accuracy: 68.23042428423594\n",
            "\n",
            "Train Epoch: 48 [0/11594 (0%)]\tLoss: 0.800781\n",
            "Train Epoch: 48 [10000/11594 (86%)]\tLoss: 0.823830\n",
            "\n",
            "Train set:  Accuracy: 68.10419182335691, \n",
            "Test set:  Accuracy: 67.85098309761987\n",
            "\n",
            "Train Epoch: 49 [0/11594 (0%)]\tLoss: 0.722850\n",
            "Train Epoch: 49 [10000/11594 (86%)]\tLoss: 0.733278\n",
            "\n",
            "Train set:  Accuracy: 68.09556667241677, \n",
            "Test set:  Accuracy: 67.12659537771646\n",
            "\n",
            "Train Epoch: 50 [0/11594 (0%)]\tLoss: 0.723900\n",
            "Train Epoch: 50 [10000/11594 (86%)]\tLoss: 0.698702\n",
            "\n",
            "Train set:  Accuracy: 68.0610660686562, \n",
            "Test set:  Accuracy: 68.26491893756467\n",
            "\n",
            "******FINAL Accuracy at Last Epoch ******\n",
            "\n",
            "Train set:  Accuracy: 68.0610660686562, \n",
            "Test set:  Accuracy: 68.26491893756467\n",
            "\n",
            "Total Time to Execute: 28 Seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q09b2yIs1wX"
      },
      "source": [
        "# Deep Learning Model - LSTM (using pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JW3d9_ScNJt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c0bde4b-0538-41df-e6ac-1e539acbff92"
      },
      "source": [
        "imput_dim = 17201\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = 3\n",
        "n_layers = 2\n",
        "dropout = 0.5\n",
        "bidirectional = True\n",
        "\n",
        "\n",
        "model = LSTM(imput_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer: SGD with learning rate of 1e-2 and momentum of 0.5\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "tr_acc_list = list()\n",
        "ts_acc_list = list()\n",
        "# Training loop with 30 epochs\n",
        "for epoch in range(1, 50 + 1):\n",
        "    \n",
        "    # Train model\n",
        "    tr_acc = train(model,device,train_loaders,optimizer,epoch)\n",
        "    tr_acc_list.append(tr_acc)\n",
        "    \n",
        "    # Test model\n",
        "    ts_acc = test(model,device,test_loaders)\n",
        "    ts_acc_list.append(ts_acc)\n",
        "    print('\\nTrain set:  Accuracy: {}, \\nTest set:  Accuracy: {}\\n'.format(\n",
        "        tr_acc,ts_acc ))\n",
        "print(\"******FINAL Accuracy at Last Epoch ******\")\n",
        "print('\\nTrain set:  Accuracy: {}, \\nTest set:  Accuracy: {}\\n'.format(\n",
        "        tr_acc,ts_acc ))\n",
        "en = time.time()\n",
        "print('Total Time to Execute: {:.0f} Seconds'.format(en-st)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/11594 (0%)]\tLoss: 1.109681\n",
            "Train Epoch: 1 [10000/11594 (86%)]\tLoss: 0.865295\n",
            "\n",
            "Train set:  Accuracy: 61.178195618423324, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Train Epoch: 2 [0/11594 (0%)]\tLoss: 0.896495\n",
            "Train Epoch: 2 [10000/11594 (86%)]\tLoss: 0.912359\n",
            "\n",
            "Train set:  Accuracy: 62.60134552354666, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Train Epoch: 3 [0/11594 (0%)]\tLoss: 1.005753\n",
            "Train Epoch: 3 [10000/11594 (86%)]\tLoss: 0.871718\n",
            "\n",
            "Train set:  Accuracy: 62.60134552354666, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Train Epoch: 4 [0/11594 (0%)]\tLoss: 0.990641\n",
            "Train Epoch: 4 [10000/11594 (86%)]\tLoss: 0.957388\n",
            "\n",
            "Train set:  Accuracy: 62.60134552354666, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Train Epoch: 5 [0/11594 (0%)]\tLoss: 0.899216\n",
            "Train Epoch: 5 [10000/11594 (86%)]\tLoss: 0.941380\n",
            "\n",
            "Train set:  Accuracy: 62.60134552354666, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Train Epoch: 6 [0/11594 (0%)]\tLoss: 0.909003\n",
            "Train Epoch: 6 [10000/11594 (86%)]\tLoss: 0.879427\n",
            "\n",
            "Train set:  Accuracy: 62.60134552354666, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Train Epoch: 7 [0/11594 (0%)]\tLoss: 0.923233\n",
            "Train Epoch: 7 [10000/11594 (86%)]\tLoss: 0.884288\n",
            "\n",
            "Train set:  Accuracy: 62.60134552354666, \n",
            "Test set:  Accuracy: 62.09037599172128\n",
            "\n",
            "Train Epoch: 8 [0/11594 (0%)]\tLoss: 0.977848\n",
            "Train Epoch: 8 [10000/11594 (86%)]\tLoss: 0.907755\n",
            "\n",
            "Train set:  Accuracy: 62.644471278247366, \n",
            "Test set:  Accuracy: 62.33183856502242\n",
            "\n",
            "Train Epoch: 9 [0/11594 (0%)]\tLoss: 0.903334\n",
            "Train Epoch: 9 [10000/11594 (86%)]\tLoss: 0.932096\n",
            "\n",
            "Train set:  Accuracy: 62.94635156115232, \n",
            "Test set:  Accuracy: 62.64229044498103\n",
            "\n",
            "Train Epoch: 10 [0/11594 (0%)]\tLoss: 0.788790\n",
            "Train Epoch: 10 [10000/11594 (86%)]\tLoss: 0.918534\n",
            "\n",
            "Train set:  Accuracy: 63.58461273072279, \n",
            "Test set:  Accuracy: 63.74611935150052\n",
            "\n",
            "Train Epoch: 11 [0/11594 (0%)]\tLoss: 0.817377\n",
            "Train Epoch: 11 [10000/11594 (86%)]\tLoss: 0.802270\n",
            "\n",
            "Train set:  Accuracy: 64.51612903225806, \n",
            "Test set:  Accuracy: 64.57399103139014\n",
            "\n",
            "Train Epoch: 12 [0/11594 (0%)]\tLoss: 0.949996\n",
            "Train Epoch: 12 [10000/11594 (86%)]\tLoss: 0.824851\n",
            "\n",
            "Train set:  Accuracy: 65.24926686217009, \n",
            "Test set:  Accuracy: 66.02276647119696\n",
            "\n",
            "Train Epoch: 13 [0/11594 (0%)]\tLoss: 0.779148\n",
            "Train Epoch: 13 [10000/11594 (86%)]\tLoss: 0.940264\n",
            "\n",
            "Train set:  Accuracy: 65.2923926168708, \n",
            "Test set:  Accuracy: 66.43670231114177\n",
            "\n",
            "Train Epoch: 14 [0/11594 (0%)]\tLoss: 0.835047\n",
            "Train Epoch: 14 [10000/11594 (86%)]\tLoss: 0.832833\n",
            "\n",
            "Train set:  Accuracy: 66.03415559772296, \n",
            "Test set:  Accuracy: 66.74715419110038\n",
            "\n",
            "Train Epoch: 15 [0/11594 (0%)]\tLoss: 0.905614\n",
            "Train Epoch: 15 [10000/11594 (86%)]\tLoss: 0.777567\n",
            "\n",
            "Train set:  Accuracy: 66.31016042780749, \n",
            "Test set:  Accuracy: 66.98861676440151\n",
            "\n",
            "Train Epoch: 16 [0/11594 (0%)]\tLoss: 0.779016\n",
            "Train Epoch: 16 [10000/11594 (86%)]\tLoss: 0.806934\n",
            "\n",
            "Train set:  Accuracy: 66.59479040883215, \n",
            "Test set:  Accuracy: 67.16109003104519\n",
            "\n",
            "Train Epoch: 17 [0/11594 (0%)]\tLoss: 0.807974\n",
            "Train Epoch: 17 [10000/11594 (86%)]\tLoss: 0.787853\n",
            "\n",
            "Train set:  Accuracy: 66.77591857857513, \n",
            "Test set:  Accuracy: 67.57502587098999\n",
            "\n",
            "Train Epoch: 18 [0/11594 (0%)]\tLoss: 0.734375\n",
            "Train Epoch: 18 [10000/11594 (86%)]\tLoss: 0.775633\n",
            "\n",
            "Train set:  Accuracy: 67.07779886148008, \n",
            "Test set:  Accuracy: 67.95446705760607\n",
            "\n",
            "Train Epoch: 19 [0/11594 (0%)]\tLoss: 0.747229\n",
            "Train Epoch: 19 [10000/11594 (86%)]\tLoss: 0.842032\n",
            "\n",
            "Train set:  Accuracy: 67.37105399344489, \n",
            "Test set:  Accuracy: 68.09244567092101\n",
            "\n",
            "Train Epoch: 20 [0/11594 (0%)]\tLoss: 0.847005\n",
            "Train Epoch: 20 [10000/11594 (86%)]\tLoss: 0.805837\n",
            "\n",
            "Train set:  Accuracy: 67.62980852164912, \n",
            "Test set:  Accuracy: 68.12694032424974\n",
            "\n",
            "Train Epoch: 21 [0/11594 (0%)]\tLoss: 0.867049\n",
            "Train Epoch: 21 [10000/11594 (86%)]\tLoss: 0.802290\n",
            "\n",
            "Train set:  Accuracy: 67.51768155942729, \n",
            "Test set:  Accuracy: 68.29941359089341\n",
            "\n",
            "Train Epoch: 22 [0/11594 (0%)]\tLoss: 0.939450\n",
            "Train Epoch: 22 [10000/11594 (86%)]\tLoss: 0.747254\n",
            "\n",
            "Train set:  Accuracy: 67.55218216318785, \n",
            "Test set:  Accuracy: 68.43739220420835\n",
            "\n",
            "Train Epoch: 23 [0/11594 (0%)]\tLoss: 0.727560\n",
            "Train Epoch: 23 [10000/11594 (86%)]\tLoss: 0.706351\n",
            "\n",
            "Train set:  Accuracy: 67.44868035190616, \n",
            "Test set:  Accuracy: 68.40289755087962\n",
            "\n",
            "Train Epoch: 24 [0/11594 (0%)]\tLoss: 0.784515\n",
            "Train Epoch: 24 [10000/11594 (86%)]\tLoss: 0.814503\n",
            "\n",
            "Train set:  Accuracy: 68.08694152147663, \n",
            "Test set:  Accuracy: 68.23042428423594\n",
            "\n",
            "Train Epoch: 25 [0/11594 (0%)]\tLoss: 0.744020\n",
            "Train Epoch: 25 [10000/11594 (86%)]\tLoss: 0.712970\n",
            "\n",
            "Train set:  Accuracy: 67.9575642573745, \n",
            "Test set:  Accuracy: 68.88582269748188\n",
            "\n",
            "Train Epoch: 26 [0/11594 (0%)]\tLoss: 0.789095\n",
            "Train Epoch: 26 [10000/11594 (86%)]\tLoss: 0.811441\n",
            "\n",
            "Train set:  Accuracy: 68.27669484215974, \n",
            "Test set:  Accuracy: 68.95481200413936\n",
            "\n",
            "Train Epoch: 27 [0/11594 (0%)]\tLoss: 0.732849\n",
            "Train Epoch: 27 [10000/11594 (86%)]\tLoss: 0.751417\n",
            "\n",
            "Train set:  Accuracy: 68.23356908745903, \n",
            "Test set:  Accuracy: 68.50638151086582\n",
            "\n",
            "Train Epoch: 28 [0/11594 (0%)]\tLoss: 0.682099\n",
            "Train Epoch: 28 [10000/11594 (86%)]\tLoss: 0.725574\n",
            "\n",
            "Train set:  Accuracy: 68.10419182335691, \n",
            "Test set:  Accuracy: 69.12728527078303\n",
            "\n",
            "Train Epoch: 29 [0/11594 (0%)]\tLoss: 0.859258\n",
            "Train Epoch: 29 [10000/11594 (86%)]\tLoss: 0.750774\n",
            "\n",
            "Train set:  Accuracy: 68.43194755908229, \n",
            "Test set:  Accuracy: 69.26526388409796\n",
            "\n",
            "Train Epoch: 30 [0/11594 (0%)]\tLoss: 0.829282\n",
            "Train Epoch: 30 [10000/11594 (86%)]\tLoss: 0.863328\n",
            "\n",
            "Train set:  Accuracy: 68.46644816284285, \n",
            "Test set:  Accuracy: 68.95481200413936\n",
            "\n",
            "Train Epoch: 31 [0/11594 (0%)]\tLoss: 0.695066\n",
            "Train Epoch: 31 [10000/11594 (86%)]\tLoss: 0.838350\n",
            "\n",
            "Train set:  Accuracy: 68.61307572882525, \n",
            "Test set:  Accuracy: 69.12728527078303\n",
            "\n",
            "Train Epoch: 32 [0/11594 (0%)]\tLoss: 0.874755\n",
            "Train Epoch: 32 [10000/11594 (86%)]\tLoss: 0.779848\n",
            "\n",
            "Train set:  Accuracy: 68.91495601173021, \n",
            "Test set:  Accuracy: 69.54122111072783\n",
            "\n",
            "Train Epoch: 33 [0/11594 (0%)]\tLoss: 0.807138\n",
            "Train Epoch: 33 [10000/11594 (86%)]\tLoss: 0.808486\n",
            "\n",
            "Train set:  Accuracy: 68.99258237019149, \n",
            "Test set:  Accuracy: 69.26526388409796\n",
            "\n",
            "Train Epoch: 34 [0/11594 (0%)]\tLoss: 0.767831\n",
            "Train Epoch: 34 [10000/11594 (86%)]\tLoss: 0.654159\n",
            "\n",
            "Train set:  Accuracy: 69.03570812489218, \n",
            "Test set:  Accuracy: 69.33425319075543\n",
            "\n",
            "Train Epoch: 35 [0/11594 (0%)]\tLoss: 0.707194\n",
            "Train Epoch: 35 [10000/11594 (86%)]\tLoss: 0.826173\n",
            "\n",
            "Train set:  Accuracy: 68.78557874762808, \n",
            "Test set:  Accuracy: 69.57571576405657\n",
            "\n",
            "Train Epoch: 36 [0/11594 (0%)]\tLoss: 0.760020\n",
            "Train Epoch: 36 [10000/11594 (86%)]\tLoss: 0.703173\n",
            "\n",
            "Train set:  Accuracy: 69.40658961531827, \n",
            "Test set:  Accuracy: 69.85167299068644\n",
            "\n",
            "Train Epoch: 37 [0/11594 (0%)]\tLoss: 0.871613\n",
            "Train Epoch: 37 [10000/11594 (86%)]\tLoss: 0.702469\n",
            "\n",
            "Train set:  Accuracy: 69.41521476625842, \n",
            "Test set:  Accuracy: 69.8171783373577\n",
            "\n",
            "Train Epoch: 38 [0/11594 (0%)]\tLoss: 0.727030\n",
            "Train Epoch: 38 [10000/11594 (86%)]\tLoss: 0.784830\n",
            "\n",
            "Train set:  Accuracy: 69.46696567189926, \n",
            "Test set:  Accuracy: 69.92066229734391\n",
            "\n",
            "Train Epoch: 39 [0/11594 (0%)]\tLoss: 0.679766\n",
            "Train Epoch: 39 [10000/11594 (86%)]\tLoss: 0.766478\n",
            "\n",
            "Train set:  Accuracy: 69.69121959634293, \n",
            "Test set:  Accuracy: 69.78268368402898\n",
            "\n",
            "Train Epoch: 40 [0/11594 (0%)]\tLoss: 0.763012\n",
            "Train Epoch: 40 [10000/11594 (86%)]\tLoss: 0.679033\n",
            "\n",
            "Train set:  Accuracy: 69.79472140762464, \n",
            "Test set:  Accuracy: 69.92066229734391\n",
            "\n",
            "Train Epoch: 41 [0/11594 (0%)]\tLoss: 0.824775\n",
            "Train Epoch: 41 [10000/11594 (86%)]\tLoss: 0.800488\n",
            "\n",
            "Train set:  Accuracy: 69.59634293600138, \n",
            "Test set:  Accuracy: 69.26526388409796\n",
            "\n",
            "Train Epoch: 42 [0/11594 (0%)]\tLoss: 0.675036\n",
            "Train Epoch: 42 [10000/11594 (86%)]\tLoss: 0.617230\n",
            "\n",
            "Train set:  Accuracy: 69.58771778506124, \n",
            "Test set:  Accuracy: 70.02414625733012\n",
            "\n",
            "Train Epoch: 43 [0/11594 (0%)]\tLoss: 0.819503\n",
            "Train Epoch: 43 [10000/11594 (86%)]\tLoss: 0.647178\n",
            "\n",
            "Train set:  Accuracy: 69.42383991719855, \n",
            "Test set:  Accuracy: 70.85201793721973\n",
            "\n",
            "Train Epoch: 44 [0/11594 (0%)]\tLoss: 0.605048\n",
            "Train Epoch: 44 [10000/11594 (86%)]\tLoss: 0.630998\n",
            "\n",
            "Train set:  Accuracy: 70.21735380369157, \n",
            "Test set:  Accuracy: 69.85167299068644\n",
            "\n",
            "Train Epoch: 45 [0/11594 (0%)]\tLoss: 0.826006\n",
            "Train Epoch: 45 [10000/11594 (86%)]\tLoss: 0.709615\n",
            "\n",
            "Train set:  Accuracy: 70.00172503018803, \n",
            "Test set:  Accuracy: 70.02414625733012\n",
            "\n",
            "Train Epoch: 46 [0/11594 (0%)]\tLoss: 0.689595\n",
            "Train Epoch: 46 [10000/11594 (86%)]\tLoss: 0.658355\n",
            "\n",
            "Train set:  Accuracy: 70.29498016215284, \n",
            "Test set:  Accuracy: 70.95550189720593\n",
            "\n",
            "Train Epoch: 47 [0/11594 (0%)]\tLoss: 0.743687\n",
            "Train Epoch: 47 [10000/11594 (86%)]\tLoss: 0.716542\n",
            "\n",
            "Train set:  Accuracy: 70.4329825771951, \n",
            "Test set:  Accuracy: 69.85167299068644\n",
            "\n",
            "Train Epoch: 48 [0/11594 (0%)]\tLoss: 0.710314\n",
            "Train Epoch: 48 [10000/11594 (86%)]\tLoss: 0.609312\n",
            "\n",
            "Train set:  Accuracy: 70.25185440745213, \n",
            "Test set:  Accuracy: 70.9210072438772\n",
            "\n",
            "Train Epoch: 49 [0/11594 (0%)]\tLoss: 0.787975\n",
            "Train Epoch: 49 [10000/11594 (86%)]\tLoss: 0.766523\n",
            "\n",
            "Train set:  Accuracy: 69.95859927548732, \n",
            "Test set:  Accuracy: 71.4384270438082\n",
            "\n",
            "Train Epoch: 50 [0/11594 (0%)]\tLoss: 0.833642\n",
            "Train Epoch: 50 [10000/11594 (86%)]\tLoss: 0.644604\n",
            "\n",
            "Train set:  Accuracy: 70.26910470933241, \n",
            "Test set:  Accuracy: 70.95550189720593\n",
            "\n",
            "******FINAL Accuracy at Last Epoch ******\n",
            "\n",
            "Train set:  Accuracy: 70.26910470933241, \n",
            "Test set:  Accuracy: 70.95550189720593\n",
            "\n",
            "Total Time to Execute: 685 Seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WQVFpK7pVlB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}